{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3, align=center>Tweet about us!</h3>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?hashtags=BDUmeetup%2C&original_referer=https%3A%2F%2Fabout.twitter.com%2Fresources%2Fbuttons&ref_src=twsrc%5Etfw&related=SaeedAghabozorg&text=%23Text%20%23Analysis%20%26%20%23NLP%20%20%40BigDataU%20%23NLP%20%23TextMining%20%23datascience%20Livestream%20here%3A%20http%3A%2F%2Fbit.ly%2FEventTextMining&tw_p=tweetbutton\"><img src = 'http://ibm.box.com/shared/static/71e9pzujwf4094sp762cib6eswf5cald.png', width=600></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#1.  Importing packages\n",
    "For this project we are going to need a variety of python modules mostly from the Natural Language Toolkit (NLTK) package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import MaxentClassifier, NaiveBayesClassifier\n",
    "import re \n",
    "import string \n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "import collections\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.lancaster import LancasterStemmer as stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In order to save memory python's nltk package replaces some of the features with \"Lazy\" versions of themselves on import. To access the full version of the Reuters corpus, the stopwords corpus, the wordnet, and the part of speech tagger we will use later on in this notebook we must use the nltk downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/evgenus/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/evgenus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /Users/evgenus/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/evgenus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"stopwords\") \n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#2. Downloading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: wget: command not found\n"
     ]
    }
   ],
   "source": [
    "!wget -O /resources/Sentiment140.200000.csv https://ibm.box.com/shared/static/cdzdpognqgn2ny54e66uoipnorxsfccg.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: wget: command not found\n"
     ]
    }
   ],
   "source": [
    "!wget -O /resources/ELXN42test2.txt  https://ibm.box.com/shared/static/ssx1isyzg7szp11u71uchn2114td8ndn.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 3. Pre-Processing The Text Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "   ###A) The Reuters Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The first major step is constructing the $\\textbf{Term-Document Matrix}$ or $\\textbf{TDM}$ for the Reuters corpus. The TDM is a very widely known and widely used tool in natural language processing. In this case study we use the TDM solely to get prior probabilities for our spell-checker but it is included because knowledge of the TDM translates well into any NLP project the reader may try. The first thing we must do with regards specifically to the Reuters corpus is write a function that can take a list of words and return only the important ones and remove any punctuation. Unimportant words in NLP are called $\\textbf{stop words}$ and can be removed with a simple reference to the nltk stopwords corpus. The function \"_clean_up2\" will be needed to create the normalized tweet training data using the Lancaster Stemmer we imported earlier, _clean_up has the added parameter in the definition as a convenience for the \"format_train\"function later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def _clean_up2(word_list, st): \n",
    "    '''Return a list of strings cleaned_words which consists of the a\n",
    "    Lancaster Stemmer st normailized words from a list of \n",
    "    strings word_list with stop words and punctuation removed'''\n",
    "    stop_words = stopwords.words('english') \n",
    "    cleaned_words = [st.stem(cleanup(word)) for word in word_list if (word not in stop_words and \n",
    "                     word not in list(string.punctuation))]\n",
    "    return cleaned_words\n",
    "\n",
    "def _clean_up(word_list, st=None): \n",
    "    '''Return a list of strings cleaned_words which consists of the words from a list of \n",
    "    strings word_list with stop words and punctuation removed'''\n",
    "    stop_words = stopwords.words('english') \n",
    "    cleaned_words = [cleanup(word) for word in word_list if (word not in stop_words and \n",
    "                     word not in list(string.punctuation))]\n",
    "    return cleaned_words\n",
    "\n",
    "def cleanup(s): \n",
    "    '''remove punctuation and unwanted characters or hyperlinks from list of string s'''\n",
    "    return s.translate(str.maketrans(string.punctuation, \" \"*(len(string.punctuation))))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The Reuters corpus is divided into 10788 documents which are various types of news articles. The nltk CorpusReader class contains a method which takes any one of these 10788 filenames and returns a list of $\\textbf{normalized}$ words used in the document. To normalize a word is to represent it by its base (e.g. dogs $\\rightarrow$ normalization $\\rightarrow$ dog)  \n",
    "\n",
    "Most of the time the TDM is actually a Matrix put in python it is almost always better to use dictionaries (called hash tables in other languages) for storing large amounts of information. So to make the TDM we can use the following function and if we need the name of a document represented at position $\\textit{j}$ we can find it by looking up the $\\textit{jth}$ name in reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def make_tdm(corpus): \n",
    "    '''Return a list of dictionaries TDM where each element contains the words\n",
    "    used and word counts in each corpus document''' \n",
    "    TDM = []\n",
    "    for f in corpus.fileids(): \n",
    "        word_list = corpus.words(f) \n",
    "        cleaned_words = _clean_up(word_list) \n",
    "        DM = Counter(cleaned_words) \n",
    "        TDM.append(DM)\n",
    "    \n",
    "    return TDM \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that we have made the TDM we can use this code heavily inspired by Peter Norvig (you can find the original here: $\\href{url}{http://norvig.com/spell-correct.html}$). The idea is we would like to estimate what word is was meant by a misspelled word we try to maximize P(intended word | input string), using the Bayes Rule $argmax_{w}P(w|s) = argmax_{w}P(s|w)*P(w)$. The goal the TDM serves is in estimating the prior probability P(w) which is the MLE for our $\\textbf{language model}$ in this example we assign equal probability to any of the five operations you can see in the \"edits1(word)\" function below. If you wanted to build a better $\\textbf{error model}$ from which to draw the probability that an author typed a string $\\textit{s}$ when he/she meant $\\textit{w}$ you could add weights for keys that are close together or use misspelled data to estimate a better distribution. \n",
    "\n",
    "For now we develop the aggregated TDM in NWORDS and we will later call the function correct(word) to spell check our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def train_tdm(tdm): \n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for doc in tdm: \n",
    "        for word in doc.keys(): \n",
    "            model[word] += doc[word]\n",
    "    return model\n",
    "\n",
    "TDM = make_tdm(reuters) \n",
    "NWORDS = train_tdm(TDM)\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits1(word):\n",
    "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "   deletes    = [a + b[1:] for a, b in splits if b]\n",
    "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
    "   return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known(words): return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or [word]\n",
    "    return max(candidates, key=NWORDS.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "###B) The ELXN42 Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that we have the spell checker implemented we can load and clean the test_data. We want to compare results of the classifiers on the clean and unclean data so we will keep both. Since here we don't get normalized words, we can also get the normalized words by using the LancasterStemmer from the nltk packagage which we imported earlier as \"stem\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"ELXN42test2.txt\") as f:\n",
    "    cdnpoli = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "clean_list = [] \n",
    "st = stem()\n",
    "for tweet in cdnpoli: \n",
    "    clean_tweet = _clean_up(tweet.split()) \n",
    "    for i in range(len(clean_tweet)): \n",
    "        if not wn.synsets(clean_tweet[i]): \n",
    "            clean_tweet[i] = correct(st.stem(clean_tweet[i]))\n",
    "    clean_string = \" \".join(clean_tweet) \n",
    "    clean_list.append(clean_string) \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#4. Subject Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Part of Speech Tagging (POST) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The second big step in this project is determining the subject and main political party referred to by a tweet. In NLP this is called the \"aboutness\" problem and there is often no easy solution. For subject determination we will be using $\\textbf{part of speech tagging (POST)}$ although I encourage the reader to take a look at $\\textbf{Semantic Role Labelling}$ in the $\\textit{nlpnet}$ package as further reading. \n",
    "\n",
    "For determining the political party in question we will try to extract relevant hashtags and then determine which of those are in the body of a tweet and if none are, using the trailing hashtags as an indicator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In getting the subject we will use the pos_tag and word_tokenize functions we imported earlier. If we can see the subjects of tweets we can use opinion mining to estimate how people feel about certain issues as well as political parties. First we implement a help function which gets all of the nouns in a given tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def _get_subjects(tweet): \n",
    "    subjects = []\n",
    "    word_list = tweet.split() \n",
    "    trail_tags = 0\n",
    "    for i in range(len(word_list)-1, -1, -1):\n",
    "        x = word_list[i]\n",
    "        if x[0]==\"#\": \n",
    "            trail_tags += 1 \n",
    "    word_list2 = word_list[:(len(word_list)-trail_tags)]\n",
    "    new_tweet = ' '.join(word_list2) \n",
    "    tagged = pos_tag(word_tokenize(new_tweet))\n",
    "    for word, tag in tagged: \n",
    "        if (tag in [\"NNP\", \"NNS\", \"PRP$\", \"NNPS\", \"NN\"]): \n",
    "            subjects.append(word) \n",
    "    \n",
    "    return subjects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Using this helper function we can construct a dictionary where each key is a noun and each value is a list of tweets which contain that noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def make_sub_dict(tweet_list): \n",
    "    d = {} \n",
    "    for tweet in tweet_list: \n",
    "        subs = _get_subjects(tweet)\n",
    "        if len(subs)> 0:  \n",
    "            for sub in subs:\n",
    "                if sub in d.keys(): \n",
    "                    d[sub].append(tweet) \n",
    "                else: \n",
    "                    d[sub] = [tweet]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now we simply make the dictionaries, one for the unprocessed test data and one for the processed. Note: this may take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/Users/evgenus/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/evgenus/anaconda3/nltk_data'\n    - '/Users/evgenus/anaconda3/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8e7ea2a097af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msub_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_sub_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdnpoli\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-b1c6bfe62ee9>\u001b[0m in \u001b[0;36mmake_sub_dict\u001b[0;34m(tweet_list)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0msubs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_subjects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-3c4ba202e0c9>\u001b[0m in \u001b[0;36m_get_subjects\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mword_list2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrail_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnew_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"NNP\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NNS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PRP$\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NNPS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/Users/evgenus/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/evgenus/anaconda3/nltk_data'\n    - '/Users/evgenus/anaconda3/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sub_d = make_sub_dict(cdnpoli) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "clean_sub_d = make_sub_dict(clean_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#5. Political Party Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Getting the relevant political party is a different process altogether. What we do is find all the hashtags using regular expressions (python's re module) and then filter the most common ones with rule-based affiliations. For example we consider any tweet where the first political hashtag or word used it \"trudeau\" to be about the Liberal Party. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "###A) Finding Top Hashtags in ELXN42 Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_hashtags(tweet_list, n): \n",
    "    '''Return the top n hashtags used in election tweet strings\n",
    "    stored in a list tweet_list'''\n",
    "    tag_list = [] \n",
    "    for tweet in tweet_list: \n",
    "        tags = re.findall(\"#\\w\\w+\", tweet) \n",
    "        tag_list += (tags) \n",
    "    tag_dict = Counter(tag_list) \n",
    "    return tag_dict.most_common(n) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "hashtag_tups = get_hashtags(cdnpoli, 600) \n",
    "hashtags = [tag for tag, value in hashtag_tups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "###B) Building a List of Party-Relevant Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then we instantiate certain partisan hashtags manually (rule-based) and look for similar common hashtags to build a larger network of partisan hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_partisan(hashtags): \n",
    "    Liberal_tags = [\"#liberal\", \"justin\", \"trudeau\", \"lpc\", \"#lpc\" \"realchage\", \"justnotready\"\n",
    "                   \"#justin\", \"#trudeau\", \"#realchage\", \"liberal\", \"#justnotready\"]\n",
    "    NDP_tags = [\"#ndp\", \"tom\", \"thomas\", \"mulcair\", \"changethatsready\"\n",
    "               \"ndp\", \"#tom\", \"#thomas\", \"#mulcair\", \"#changethatsready\"]\n",
    "    Tory_tags = [\"#conservative\", \"steve\", \"steven\" \"harper\", \"tory\",\n",
    "                \"#steve\", \"#steven\" \"#harper\", \"#tory\", \"conservative\", \"cpc\", \"#cpc\"]\n",
    "    partisan = [Tory_tags, NDP_tags, Liberal_tags]\n",
    "    for tag in hashtags:\n",
    "        for party in partisan: \n",
    "            for string in party: \n",
    "                if string in tag and tag not in party: \n",
    "                    party.append(tag) \n",
    "                    party.append(tag[1:])\n",
    "                    \n",
    "    return partisan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now we can use the hashtags we extracted earlier to make a list of lists of partisan hashtags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "p = get_partisan(hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "###C) Determining A Likely Political Party By Tweet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now to get the party referred to by a tweet we look at the first partisan word or tag used in the tweet using the following helper function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def _get_party(tweet, partisan):\n",
    "    '''Return a string word which is our best guess for the subject of \n",
    "    a string tweet using our list hashtags if we have no guess return the \n",
    "    string \"unkown\"'''\n",
    "    tagged = pos_tag(word_tokenize(tweet))\n",
    "    for word, tag in tagged:\n",
    "        for party in partisan:\n",
    "            if word in party: \n",
    "                return party[0]\n",
    "\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As before with the subject dictionary, we use a dictionary where each key is a political party (or \"unknown\") and each value is a list of relevant tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def make_party_dict(tweet_list, partisan): \n",
    "    '''Return a Dictionary where each key is a subject and each \n",
    "    value a list of tweets with that subject'''\n",
    "    tweet_dict = {} \n",
    "    for tweet in tweet_list: \n",
    "        sub = _get_party(tweet, partisan) \n",
    "        if sub in tweet_dict.keys(): \n",
    "            tweet_dict[sub].append(tweet)\n",
    "        else: \n",
    "            tweet_dict[sub] = []\n",
    "            tweet_dict[sub].append(tweet)\n",
    "            \n",
    "    return tweet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "button": false,
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "par_d = make_party_dict(cdnpoli, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "clean_par_d = make_party_dict(clean_list, p) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#6. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "###A) Formatting The Sentiment140 Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First we must load and format the data. nltk.classify.classifiers take training input in the following form [({feature1: value, feature2: value, feature3: value, ....}, category}), ({feature1: value, feature2: value, feature3: value, ....}, category})] in other words, a list of tuples where each entry is a training case and its class label. In our case the features will be words and the values their respective counts. We also allow the choice of cleaning functions for later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def format_train(trainData, clean_function, st=None):\n",
    "    '''Return a list of Tuples of tweets in csv.DictReader trainData\n",
    "    where the first entry is a dictionary of words in a tweet and the \n",
    "    value \"True\" and the second is the class value we choose the \n",
    "    cleaning function from _clean_up and _clean_up2'''\n",
    "    tweet_list = [] \n",
    "    for row in trainData: \n",
    "        tweet = row[\"text\"]\n",
    "        twl = tweet.split()\n",
    "        twl = clean_function(twl, st)\n",
    "        cat = row[\"class\"]\n",
    "        features = {}\n",
    "        for word in twl: \n",
    "            features[word] = True\n",
    "        if cat == \"0\": \n",
    "            tup = (features, \"negative\") \n",
    "        else: \n",
    "            tup = (features, \"positive\") \n",
    "        tweet_list.append(tup) \n",
    "    return tweet_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now we load and format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open('/resources/Sentiment140.200000.csv', 'rb') as csvfile:\n",
    "    trainData = csv.DictReader(csvfile) \n",
    "    dat = format_train(trainData, _clean_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And for the cleaned data we use _clean_up2 instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "button": false,
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open('/resources/Sentiment140.200000.csv', 'rb') as csvfile:\n",
    "    trainData = csv.DictReader(csvfile) \n",
    "    st = stem()\n",
    "    dat2 = format_train(trainData, _clean_up2, st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "###B) Training the Classifiers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In machine learning algorithms we often want to test the performance of a classifier on known examples so that we can get a grasp on how confident we should be in the results on the data we are really concerned with this. We do this by a process called $\\textbf{cross validation}$ wherein we divide our training data further into a training set and labelled testing set. Since we have two categories, we would like an equal amount of training cases for both which we can implement in the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (3 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.500\n",
      "             2          -0.69314        0.868\n",
      "         Final          -0.69313        0.868\n"
     ]
    }
   ],
   "source": [
    "positive = dat[100000:]\n",
    "negative = dat[:100000]\n",
    "train1 = positive[:80000] + negative[:80000]\n",
    "test = positive[80000:] + negative[80000:] \n",
    "algorithm = MaxentClassifier.ALGORITHMS[0]\n",
    "MEclassifier = MaxentClassifier.train(train1, algorithm,max_iter=3)\n",
    "NBclassifier = NaiveBayesClassifier.train(train1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And we do the same for the cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (3 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.500\n",
      "             2          -0.69314        0.847\n",
      "         Final          -0.69313        0.847\n"
     ]
    }
   ],
   "source": [
    "positive2 = dat2[100000:]\n",
    "negative2 = dat2[:100000]\n",
    "train2 = positive2[:80000] + negative2[:80000]\n",
    "test2 = positive2[80000:] + negative2[80000:] \n",
    "algorithm2 = MaxentClassifier.ALGORITHMS[0]\n",
    "MEclassifier2 = MaxentClassifier.train(train2, algorithm,max_iter=3)\n",
    "NBclassifier2 = NaiveBayesClassifier.train(train2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "###C) Testing The Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can then test the performance of our classifier by the $\\textbf{categorization error}$ which is the percentage of the labelled test set cases it misclassifies, the functin is easy enough to write and uses the nltk.classify.classifier class' method \"classify(case)\" as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_cat_error(test, classifier): \n",
    "    '''Return float cat_error which is the categorization error \n",
    "    of an nltk.classify classifier on a testset test which is a \n",
    "    list of tuples with the first entry being a dictionary of \n",
    "    features and the second a string denoting a category'''\n",
    "    wrong = 0 \n",
    "    for case in test: \n",
    "        cla = classifier.classify(case[0]) \n",
    "        if cla != case[1]: \n",
    "            wrong += 1 \n",
    "    \n",
    "    error = float(wrong)/float(len(test))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25345\n",
      "0.257525\n"
     ]
    }
   ],
   "source": [
    "print get_cat_error(test, MEclassifier) \n",
    "print get_cat_error(test, NBclassifier) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#7. Getting Our Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The fourth and final major step in this project is getting our actual results. Now that we have everything made we will simply need to find the percentage of positive tweets for a certain key value in one of the 4 dictionaries we constructed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "###A) Results By Political Party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_percent_positive(key, d, classifier):\n",
    "    pos_count = 0 \n",
    "    for tweet in d[key]: \n",
    "        cla = classifier.classify(Counter(tweet)) \n",
    "        if cla == \"positive\": \n",
    "            pos_count += 1\n",
    "    \n",
    "    return 100*float(pos_count)/len(d[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now to get the results by party we can simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Positive Total Tweets\n",
      "#ndp 54.7008547009 234\n",
      "#liberal 45.6035767511 671\n",
      "#conservative 43.8066465257 331\n",
      "Percent Positive Total Tweets\n",
      "#ndp 51.7094017094 234\n",
      "#liberal 43.9642324888 671\n",
      "#conservative 44.1087613293 331\n"
     ]
    }
   ],
   "source": [
    "for classifier in [MEclassifier, NBclassifier]: \n",
    "    print \"Percent Positive\", \"Total Tweets\"\n",
    "    for party in [\"#ndp\", \"#liberal\", \"#conservative\"]: \n",
    "        print party, get_percent_positive(party, par_d, classifier), len(par_d[party])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And for the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Positive Total Tweets\n",
      "#ndp 53.5269709544 241\n",
      "#liberal 59.3917710197 559\n",
      "#conservative 48.7179487179 156\n",
      "Percent Positive Total Tweets\n",
      "#ndp 53.5269709544 241\n",
      "#liberal 59.3917710197 559\n",
      "#conservative 48.7179487179 156\n"
     ]
    }
   ],
   "source": [
    "for classifier in [MEclassifier2, NBclassifier2]: \n",
    "    print \"Percent Positive\", \"Total Tweets\"\n",
    "    for party in [\"#ndp\", \"#liberal\", \"#conservative\"]: \n",
    "        print party, get_percent_positive(party, clean_par_d, classifier), len(clean_par_d[party])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "###B) Results By Subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can similarly classify tweets by subject using the other two dictionaries we constructed previously. Since there are so many subjects we can restrict them to a certain sample in the interest of brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "some_keys = [\"jobs\", \"pm\", \"women\", \"realchange\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Positive Total Tweets\n",
      "jobs 77.2727272727 22\n",
      "pm 29.6296296296 27\n",
      "women 52.380952381 21\n",
      "realchange 47.4576271186 59\n",
      "Percent Positive Total Tweets\n",
      "jobs 77.2727272727 22\n",
      "pm 29.6296296296 27\n",
      "women 52.380952381 21\n",
      "realchange 42.3728813559 59\n"
     ]
    }
   ],
   "source": [
    "for classifier in [MEclassifier, NBclassifier]: \n",
    "    print \"Percent Positive\", \"Total Tweets\"\n",
    "    for sub in some_keys: \n",
    "        print sub, get_percent_positive(sub, sub_d, classifier), len(sub_d[sub])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And for the cleaned data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Positive Total Tweets\n",
      "job 68.4210526316 19\n",
      "pm 61.7647058824 34\n",
      "women 47.619047619 21\n",
      "realchang 58.2677165354 127\n",
      "Percent Positive Total Tweets\n",
      "job 68.4210526316 19\n",
      "pm 61.7647058824 34\n",
      "women 42.8571428571 21\n",
      "realchang 58.2677165354 127\n"
     ]
    }
   ],
   "source": [
    "some_keys = [\"job\", \"pm\", \"women\", \"realchang\"]\n",
    "for classifier in [MEclassifier2, NBclassifier2]: \n",
    "    print \"Percent Positive\", \"Total Tweets\"\n",
    "    for sub in some_keys: \n",
    "        print sub, get_percent_positive(sub, clean_sub_d, classifier), len(clean_sub_d[sub])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "So we see that with all kinds of data the NDP leads in terms of positive public opinion but the Liberals have the highest volume of relevant tweets by far. If we look at it by subject, we see that tweets related to \"jobs\" are incredibly positive and those relating to women are distinctly negative (if you take a look at these tweets with sub_d[\"women\"] you will see that they are mostly about women's rights and are mostly negative).\n",
    "\n",
    "As for clean vs. unclean data, we can see that the normalization process may over-restrict our ability to find party relevant tweets, however it seems to enhance our ability to filter by subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And that concludes the project! Feel free to analyze some of the tweets to see how well you think the classifiers did. The only real way to get a concrete grasp on the actual sentiment is to read them yourself I'm afraid but randomly sample a few tweets from each dictionary key and you can see that it fairly accurately reflects the sentiment of the tweet. \n",
    "\n",
    "Thank you for reading! Best of luck in all of your future NLP endeavors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3, align=center>We are sponsoring a SportsHack!</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href = \"http://sportshackweekend.org/ca/2015/\"><img src = \"http://sportshackweekend.org/ca/2015/img/ca2015.jpg\", width = 400></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3, align=center>Tweet about us!</h3>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?hashtags=BDUmeetup%2C&original_referer=https%3A%2F%2Fabout.twitter.com%2Fresources%2Fbuttons&ref_src=twsrc%5Etfw&related=SaeedAghabozorg&text=%23Text%20%23Analysis%20%26%20%23NLP%20%20%40BigDataU%20%23NLP%20%23TextMining%20%23datascience%20Livestream%20here%3A%20http%3A%2F%2Fbit.ly%2FEventTextMining&tw_p=tweetbutton\"><img src = 'http://ibm.box.com/shared/static/71e9pzujwf4094sp762cib6eswf5cald.png', width=600></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
